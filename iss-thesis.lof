\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Overview of the \gls {6dof} pose estimation\relax }}{9}{figure.caption.5}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Overwiew of different types of generative models\relax }}{17}{figure.caption.22}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Forward and reverse process of diffusion model using \gls {2d} image as example\relax }}{20}{figure.caption.27}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Super resolution using diffusion model, image taken from the project page of SR3\cite {saharia2021image}\relax }}{22}{figure.caption.34}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Point cloud generation using diffusion model\relax }}{22}{figure.caption.35}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Image synthesis with text prompt in different styles using Stable Diffusion\relax }}{23}{figure.caption.40}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces \gls {3d} model (\gls {nerf}) synthesis prompt using DreamFusion\relax }}{23}{figure.caption.41}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Structures of some common \gls {6dof} pose estimation methods,\relax }}{28}{figure.caption.42}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Overview of the network from the work "Confronting Ambiguity in 6D Object Pose Estimation via Score-Based Diffusion on SE(3)", image adapted from the original paper \cite {hsiao2023confronting}\relax }}{32}{figure.caption.51}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Overview of the network from PoseDiffusion, image adapted from the original paper \cite {wang2023pd}\relax }}{33}{figure.caption.52}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Structure of the training phase of the pose hypotheses diffusion\relax }}{36}{figure.caption.53}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Structure of the sampling phase of the pose hypotheses diffusion\relax }}{37}{figure.caption.54}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Denoiser network with backbone and feature extractor\relax }}{39}{figure.caption.55}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces Multi-head self-attention and scaled dot-product attention\relax }}{40}{figure.caption.56}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces The 64-dimensional positional encoding for a sequence with length of 100\relax }}{41}{figure.caption.57}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces Self-supervised architecture of DINO\relax }}{42}{figure.caption.58}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces Sturcture of the \gls {vit} model\relax }}{42}{figure.caption.59}%
\contentsline {figure}{\numberline {4.8}{\ignorespaces \gls {3d} feature extraction with FoldingNet based point cloud completion\relax }}{44}{figure.caption.60}%
\contentsline {figure}{\numberline {4.9}{\ignorespaces Structure of the feature fusion process using (a): addition of the conditional features and embedding; (b): concatination of the conditional features and embedding\relax }}{45}{figure.caption.61}%
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{45}{subfigure.9.1}%
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{45}{subfigure.9.2}%
\contentsline {figure}{\numberline {4.10}{\ignorespaces Illustration of the residual translation estimation (projected in \gls {2d} space)\relax }}{46}{figure.caption.62}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces LIMEMOD dataset. Left: training data with Synthetic \gls {cad} Model; Middle: training data rendered with \gls {pbr}-BlenderProc4BOP; Right: test data captured with Kinect Sensor\relax }}{49}{figure.caption.63}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces Comparison of \gls {lmo} dataset with \gls {lm} dataset. Left: low-level occluded scene; Right: high-level occluded scene\relax }}{50}{figure.caption.64}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Visualization of the attention maps of the input image, where 12 heatmaps represent 12 attension heads output in transformer. Top: hole puncher; Middle: glue; Bottem: phone (occluded)\relax }}{54}{figure.caption.67}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Attension maps of the different scale of \glspl {vit} and patch size. From left to right: \gls {vit}-S/16, \gls {vit}-S/8, \gls {vit}-B/16, \gls {vit}-B/8. Each column represents 3 random attention heads.\relax }}{55}{figure.caption.68}%
\contentsline {figure}{\numberline {5.5}{\ignorespaces \gls {cd} of the training and validation dataset during the point completion pretraining\relax }}{55}{figure.caption.69}%
\contentsline {figure}{\numberline {5.6}{\ignorespaces Categorical level \gls {cd} distance distribution of the test dataset\relax }}{56}{figure.caption.70}%
\contentsline {figure}{\numberline {5.7}{\ignorespaces Point cloud completion results of the FoldingNet. Top: cup; Bottom: phone. From left to right: estimation(viewpoint 1), ground truth(viewpoint 1), estimation(viewpoint 2), ground truth(viewpoint 2). Blue points are the input partial point cloud.\relax }}{57}{figure.caption.71}%
\contentsline {figure}{\numberline {5.8}{\ignorespaces Comparision of the \gls {cd} distributions between the model trained with the whole data set and the overfitted model on single object (the first 7 objects of LM are shown)\relax }}{58}{figure.caption.72}%
\contentsline {figure}{\numberline {5.9}{\ignorespaces L2 loss of the training phase of the diffusion model. Original data (gray) is downsampled and smoothed with a $2^{nd}$ order filter (orange) for visualization. \relax }}{60}{figure.caption.76}%
\contentsline {figure}{\numberline {5.10}{\ignorespaces Categorical MSPD and MSSD recall rates with different thresholds on LM dataset (without refinement)\relax }}{63}{figure.caption.81}%
\contentsline {figure}{\numberline {5.11}{\ignorespaces Rotation error under different representations of rotation during training phase (without refinement).\relax }}{67}{figure.caption.93}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {A.1}{\ignorespaces Point cloud completion results of the FoldingNet. From top to bottom: cup, camera, phone, egg box, glue. From left to right: estimation(viewpoint 1), ground truth(viewpoint 1), estimation(viewpoint 2), ground truth(viewpoint 2). Blue points are the input partial point cloud.\relax }}{74}{figure.caption.95}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
