\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Overview of the 6 DoF pose estimation\relax }}{3}{figure.caption.2}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Overwiew of different types of generative models\relax }}{11}{figure.caption.19}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Forward and reverse process of diffusion model using 2D image as example\relax }}{14}{figure.caption.24}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Super resolution using diffusion model\relax }}{15}{figure.caption.31}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Point cloud generation using diffusion model\relax }}{16}{figure.caption.32}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Image synthesis with text prompt in different styles using Stable Diffusion\relax }}{17}{figure.caption.37}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces 3D model (NeRF) synthesis prompt using DreamFusion\relax }}{17}{figure.caption.38}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Structures of some common 6 DoF pose estimation methods,\relax }}{22}{figure.caption.39}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Overview of the network from the work "Confronting Ambiguity in 6D Object Pose Estimation via Score-Based Diffusion on SE(3)", image adapted from the original paper \cite {hsiao2023confronting}\relax }}{26}{figure.caption.48}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Overview of the network from PoseDiffusion, image adapted from the original paper \cite {wang2023pd}\relax }}{27}{figure.caption.49}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Structure of the training phase of the pose hypotheses diffusion\relax }}{30}{figure.caption.50}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Structure of the sampling phase of the pose hypotheses diffusion\relax }}{31}{figure.caption.51}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Denoiser network with backbone and feature extractor\relax }}{33}{figure.caption.52}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces Multi-head self-attention and scaled dot-product attention\relax }}{34}{figure.caption.53}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces The 64-dimensional positional encoding for a sequence with length of 100\relax }}{35}{figure.caption.54}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces Self-supervised architecture of DINO\relax }}{36}{figure.caption.55}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces Sturcture of the ViT model\relax }}{36}{figure.caption.56}%
\contentsline {figure}{\numberline {4.8}{\ignorespaces 3D feature extraction with FoldingNet based point cloud completion\relax }}{38}{figure.caption.57}%
\contentsline {figure}{\numberline {4.9}{\ignorespaces Structure of the feature fusion process using (a): addition of the conditional features and embedding; (b): concatination of the conditional features and embedding\relax }}{39}{figure.caption.58}%
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{39}{subfigure.9.1}%
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{39}{subfigure.9.2}%
\contentsline {figure}{\numberline {4.10}{\ignorespaces Illustration of the residual translation estimation (projected in 2D space)\relax }}{40}{figure.caption.59}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces LIMEMOD dataset. Left: training data with Synthetic CAD Model; Middle: training data rendered with PBR-BlenderProc4BOP; Right: test data captured with Kinect Sensor\relax }}{43}{figure.caption.60}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces Comparison of LINEMOD-O dataset with LINEMOD dataset. Left: low-level occluded scene; Right: high-level occluded scene\relax }}{44}{figure.caption.61}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Visualization of the attention maps of the input image, where 12 heatmaps represent 12 attension heads output in transformer. Top: hole puncher; Middle: glue; Bottem: phone (occluded)\relax }}{48}{figure.caption.64}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Attension maps of the different scale of ViTs and patch size. From left to right: ViT-S/16, ViT-S/8, ViT-B/16, ViT-B/8. Each column represents 3 random attention heads.\relax }}{49}{figure.caption.65}%
\contentsline {figure}{\numberline {5.5}{\ignorespaces Chamfer distance of the training and validation dataset during the point completion pretraining\relax }}{50}{figure.caption.66}%
\contentsline {figure}{\numberline {5.6}{\ignorespaces Categorical level Chamfer distance distribution of the test dataset\relax }}{50}{figure.caption.67}%
\contentsline {figure}{\numberline {5.7}{\ignorespaces Comparision of the Chamfer distance distributions between the model trained with the whole data set and the overfitted model on single object (the first 7 objects of LM are shown)\relax }}{51}{figure.caption.68}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
